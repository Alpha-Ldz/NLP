{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DlzvXLQuoTY"
      },
      "source": [
        "# Sentiment classifier with LSTM\n",
        "\n",
        "In this notebook, we will implement a simple sentiment classifier using an LSTM. Follow the notebook, complete the missing part, answer the questions and apply the asked modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utTvPhcyTC7d"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_v62rH_W9J1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import re\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from string import punctuation\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzAHrPfReDqu"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "Using the datasets library, we load the imdb dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBb40sTOXYz-"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDq8WhU3XlOR"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otcTAvKDCaZs"
      },
      "outputs": [],
      "source": [
        "# We do not need the \"unsupervised\" split.\n",
        "dataset.pop(\"unsupervised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zv5hRlcCaZv"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXssc1MjX5v2"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UoMzhBOeGxe"
      },
      "source": [
        "## Pretreatment **(1 point)**\n",
        "\n",
        "Code the `pretreatment` function which clean the input text. Look at the dataset and deduce which treatment is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw3lht4AZHQB"
      },
      "outputs": [],
      "source": [
        "def pretreatment(text: str) -> str:\n",
        "    \"\"\"Clean IMDB text entries.\n",
        "    Args:\n",
        "        text: an input string.\n",
        "    Returns:\n",
        "        The cleaned text.\n",
        "    \"\"\"\n",
        "    occ = [\"\\\\\", \"<br />\"]\n",
        "    for elm in occ:\n",
        "        text = text.replace(elm, '').translate(str.maketrans('', '', punctuation)).lower()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnxsNRj1b4ef"
      },
      "outputs": [],
      "source": [
        "# This applies the pretreatment function to all\n",
        "clean_dataset = dataset.map(lambda x: {\"text\": pretreatment(x[\"text\"]), \"label\": x[\"label\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITYcafjvZ4V"
      },
      "source": [
        "Let's see what the text now look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss9fNfoWUAz_"
      },
      "outputs": [],
      "source": [
        "clean_dataset[\"train\"][\"text\"][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sAscTs3U2Sn"
      },
      "source": [
        "Let's take a quick look at the labels. Notice that the labels are ordered in the training set starting by the negative reviews (0), followed by the positive ones (1). Training neural networks on this kind of configuration tends to considerably affect their performances. So the dataset will have to be shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZedAEQpdstA"
      },
      "outputs": [],
      "source": [
        "clean_dataset[\"train\"][\"label\"][12490:12510]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv3rt4ZieQbN"
      },
      "source": [
        "## Train/validation split **(1 point)**\n",
        "\n",
        "In our example, we consider the test split as production data. Which means, we need to treat it as if we never see it during the training process. To experiment on the model, we need to split the training set into a training and validation set. See [here](https://huggingface.co/course/chapter5/3?fw=pt#creating-a-validation-set) on how to do so with the `Datasets` library.\n",
        "\n",
        "Don't forget to **stratify** your split (we need to have the same proportion of class in both training and validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QIBkz-0YAO7"
      },
      "outputs": [],
      "source": [
        "train_dataset_clean = clean_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F58TBVUbVfWb"
      },
      "outputs": [],
      "source": [
        "train_dataset_clean\n",
        "\n",
        "clean_dataset[\"validation\"] = train_dataset_clean['test']\n",
        "clean_dataset[\"train\"] = train_dataset_clean['train']\n",
        "clean_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if the DataSet is stratified"
      ],
      "metadata": {
        "id": "gbDsgcYVZnps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w97hMnYEeexj"
      },
      "outputs": [],
      "source": [
        "tmp = clean_dataset['train']['label']\n",
        "tmp.count(0) / tmp.count(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = clean_dataset['validation']['label']\n",
        "tmp.count(0) / tmp.count(1)"
      ],
      "metadata": {
        "id": "7AAsIkyiTBDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The can see the dataset is stratified with an equal ratio in the train and the test"
      ],
      "metadata": {
        "id": "jYp4krfoTf46"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t40J_xte1dz"
      },
      "source": [
        "## Categorical encoding of the vocabulary **(2 points)**\n",
        "\n",
        "We can't feed word to a neural network. A usual solution is to turn words into categorical data by using one-hot encoding. To avoid an explosion in vocabulary size, we will only keep words which appear more than a certain amount of time.\n",
        "\n",
        "The `Vocabulary` class below will do that for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2P9A4j8aarj"
      },
      "outputs": [],
      "source": [
        "UNK_TOKEN = \"<UNK>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Vocabulary manager on a collection.\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"No parameters to provide.\n",
        "        \"\"\"\n",
        "        # Index to word mapping.\n",
        "        self.index2word = [PAD_TOKEN, UNK_TOKEN]\n",
        "        # Word to index mapping.\n",
        "        self.word2index = {value: key for key, value in enumerate(self.index2word)}\n",
        "        # Word counter.\n",
        "        self.word2count = defaultdict(int)\n",
        "\n",
        "    def add_word(self, word: str) -> None:\n",
        "        \"\"\"Increments the count of a word to the vocabulary.\n",
        "        Args:\n",
        "            word: the word.\n",
        "        \"\"\"\n",
        "        self.word2count[word] += 1\n",
        "        if not word in self.word2index:\n",
        "            self.word2index[word] = len(self.index2word)\n",
        "            self.index2word.append(word)\n",
        "\n",
        "    def add_text(self, text: str, separator: str =\" \") -> None:\n",
        "        \"\"\"Add the words given in a text to our vocabulary.\n",
        "        Args:\n",
        "            text: a sequence of words separated by a given separator.\n",
        "            separator: the separator used to split our text (default is \" \").\n",
        "        \"\"\"\n",
        "        for word in text.split(separator):\n",
        "            self.add_word(word)\n",
        "\n",
        "    def get_index(self, word: str) -> int:\n",
        "        \"\"\"Returns the index of a given word in our vocabulary.\n",
        "        If the word is not in the vocabulary, returns the index for UNK_TOKEN.\n",
        "        Args:\n",
        "            word: a string.\n",
        "        Returns:\n",
        "            The corresponding index or the index for UNK_TOKEN.\n",
        "        \"\"\"\n",
        "        return (\n",
        "            self.word2index[word]\n",
        "            if word in self.word2index\n",
        "            else self.word2index[UNK_TOKEN]\n",
        "        )\n",
        "\n",
        "    def get_word(self, index: int) -> str:\n",
        "        \"\"\"Returns the word at a given index in our vocabulary.\n",
        "        Args:\n",
        "            index: the word position in our vocabulary.\n",
        "        Returns:\n",
        "            The word corresponding to the given index.\n",
        "        \"\"\"\n",
        "        return self.index2word[index]\n",
        "\n",
        "    def get_word_count(self, word: str) -> int:\n",
        "        \"\"\"Returns the number of occurences for a given word.\n",
        "        Raise a \n",
        "        Args:\n",
        "            The word.\n",
        "        Returns:\n",
        "            Its number of measured occurences.\n",
        "        \"\"\"\n",
        "        return self.word2count[word]\n",
        "\n",
        "    def get_vocabulary(self) -> List[str]:\n",
        "        \"\"\"Returns a copy of the whole vocabulary list.\n",
        "        Returns:\n",
        "            A list of words.\n",
        "        \"\"\"\n",
        "        return deepcopy(self.index2word)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"len() function.\n",
        "        Returns:\n",
        "            The number of words in the vocabulary.\n",
        "        \"\"\"\n",
        "        return len(self.index2word)\n",
        "\n",
        "    def trim_vocabulary(self, min_occurences: int = 5) -> None:\n",
        "        \"\"\"Trim the vocabulary based on the number of occurrences of each words.\n",
        "        Note that whole counts of deleted words are added to the UNK_TOKEN counts.\n",
        "        Args:\n",
        "            min_occurences: the minimum number of occurences for a word to be kept.\n",
        "        \"\"\"\n",
        "        to_delete = {\n",
        "            word for word, count in self.word2count.items() if count < min_occurences\n",
        "        }\n",
        "        new_word2count = defaultdict(int)\n",
        "        for word, count in self.word2count.items():\n",
        "            if word not in to_delete:\n",
        "                new_word2count[word] = count\n",
        "            else:\n",
        "                new_word2count[UNK_TOKEN] += count\n",
        "        new_index2word = [word for word in self.index2word if word not in to_delete]\n",
        "        new_word2index = {word: index for index, word in enumerate(new_index2word)}\n",
        "\n",
        "        self.word2count = new_word2count\n",
        "        self.index2word = new_index2word\n",
        "        self.word2index = new_word2index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXYEmjSFCaZ6"
      },
      "source": [
        "**(1 point)** Get the vocabulary on both the training and validation set using the `Vocabulary` class. Remember, we don't use the test set here as we consider it as proxy production data. The trim it down as you see fit (around 20K words in the vocabulary is a good value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8eqcMYzfLJr"
      },
      "outputs": [],
      "source": [
        "vocabulary = Vocabulary()\n",
        "\n",
        "for text in clean_dataset['train']['text'] :\n",
        "  vocabulary.add_text(text)\n",
        "\n",
        "for text in clean_dataset['validation']['text'] :\n",
        "  vocabulary.add_text(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWPHgAHjCaZ7"
      },
      "source": [
        "**(1 point)** Fill the encoding and decoding functions. The encoding function takes a text as input and returns a list IDs corresponding to the index of each word in the vocabulary. The decoding function reverse the process, turning a list of IDs into a text. Make sure the encoding function returns a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mj4AMqYk6xt"
      },
      "outputs": [],
      "source": [
        "# Encoding and decoding function\n",
        "\n",
        "def encode_text(text: str) -> np.ndarray:\n",
        "    rtn = []\n",
        "\n",
        "    for word in text.split(' '):\n",
        "      rtn.append(vocabulary.get_index(word))\n",
        "    \n",
        "    return rtn\n",
        "\n",
        "def decode_text(encoded_text: np.ndarray) -> str:\n",
        "    rtn = \"\"\n",
        "\n",
        "    for index in range(len(encoded_text) - 1):\n",
        "      rtn += vocabulary.get_word(encoded_text[index])\n",
        "      rtn += \" \"\n",
        "\n",
        "    rtn += vocabulary.get_word(encoded_text[-1])\n",
        "\n",
        "    return rtn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPUej2whwaXw"
      },
      "source": [
        "To make sure everything went well, we compare a text before and after encoding and then decoding it. You should see rare words / typos replaced by the `<UNK>` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT8UQgeWliQz"
      },
      "outputs": [],
      "source": [
        "# Apply the encoding function to the entire dataset.\n",
        "encoded_dataset = clean_dataset.map(lambda x: {\"text\": encode_text(x[\"text\"]), \"label\": x[\"label\"]})\n",
        "\n",
        "clean_dataset[\"train\"][\"text\"][0], decode_text(encoded_dataset[\"train\"][\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVadYQD2nw52"
      },
      "source": [
        "## Batch preparation **(1 point)**\n",
        "\n",
        "To speed up learning, and take advantage of the GPU architecture, we provide data to the model by batches. Since all line in the same batch need to have the same length, we pad lines to the maximum length of each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWV2pzgqa1cD"
      },
      "outputs": [],
      "source": [
        "def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int = 32, pad_right: bool = False) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Generate randomly ordered batches of data+labels.\n",
        "    Args:\n",
        "        X: the input data.\n",
        "        y: the corresponding labels.\n",
        "        batch_size: the size of each batch [32].\n",
        "        pad_right: if true, the padding is done on the right [False].\n",
        "    \"\"\"\n",
        "    \n",
        "    X, y = shuffle(X, y)\n",
        "    n_batches = int(np.ceil(len(y) / batch_size))\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        \n",
        "        end = min((i+1)*batch_size, len(y))\n",
        "        \n",
        "        X_batch = X[i*batch_size:end]\n",
        "        y_batch = y[i*batch_size:end]\n",
        "\n",
        "        # Padding to max ength size within the batch\n",
        "        max_len = np.max([len(x) for x in X_batch])\n",
        "        for j in range(len(X_batch)):\n",
        "            x = X_batch[j]\n",
        "            pad = [vocabulary.get_index(PAD_TOKEN)] * (max_len - len(x))\n",
        "            X_batch[j] = x+pad if pad_right else pad+x\n",
        "\n",
        "        X_batch = torch.from_numpy(np.array(X_batch)).long()\n",
        "        y_batch = torch.from_numpy(np.array(y_batch)).long()\n",
        "\n",
        "        # Yielding results, so every time the function is called, it starts again from here.\n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eigWpkEaCaZ9"
      },
      "source": [
        "Let's see what the batches look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1-D1ueJb6IR"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"]):\n",
        "    print(\"inputs\", inputs, \"shape:\", inputs.shape)\n",
        "    print(\"labels\", labels, \"shape:\", labels.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAJrJGMqCaZ-"
      },
      "source": [
        "**(1 point)** Question: On which side should we pad the data for our use case and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-wfWxP0FiKPx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5uO-cAWodA0"
      },
      "source": [
        "## The model **(13 points)**\n",
        "\n",
        "We use a simple RNN with a configurable number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xel73Svtcgrf"
      },
      "outputs": [],
      "source": [
        "# Before starting, let's set up the device. A GPU if available, else the CPU.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiNVffK_cnxM"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    \"\"\"A simple RNN module with word embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: vocabulary size.\n",
        "            embed_size: embedding dimensions.\n",
        "            hidden_size: hidden layer size.\n",
        "            n_layers: the number of layers.\n",
        "            n_outputs: the number of output classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_outputs = n_outputs\n",
        "\n",
        "\n",
        "        # The word embedding layer.\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        # The RNN\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size = self.embed_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.n_layers,\n",
        "            batch_first = True, # Changes the order of dimension to put the batches first.\n",
        "        )\n",
        "        # A fully connected layer to project the RNN's output to only one output used for classification.\n",
        "        self.fc = nn.Linear(self.hidden_size, self.n_outputs)\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Function called when the model is called with data as input.\n",
        "        Args:\n",
        "            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n",
        "        Returns:\n",
        "            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        out = self.embed(X)\n",
        "        # out contains the output layer of all words in the sequence.\n",
        "        # First dim is batch, second the word in the sequence, third is the vector itself.\n",
        "        # The second output value is the last vector of all intermediate layer.\n",
        "        # Only use it if you want to access the intermediate layer values of a\n",
        "        # multilayer model.\n",
        "        out, _ = self.rnn(out, h0)\n",
        "        # Getting the last value only.\n",
        "        out = out[:, -1, :]\n",
        "    \n",
        "        # Linear projection.\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3raofAlCaaA"
      },
      "source": [
        "Note that we do not pass the output through a sigmoid function. This is because pyTorch implements some code optimization within the `BCEWithLogitsLoss` we'll see later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw-lzgfveXB3"
      },
      "outputs": [],
      "source": [
        "def accuracy(\n",
        "    model: nn.Module,\n",
        "    train_gen: Callable,\n",
        "    valid_gen: Callable\n",
        ") :\n",
        "        model.train()\n",
        "        model.eval()\n",
        "\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in train_gen():\n",
        "\n",
        "                labels = labels.view(-1, 1).float()\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                outputs = torch.sigmoid(outputs)\n",
        "                outputs = (outputs > 0.5).float()\n",
        "                train_correct += (outputs == labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "\n",
        "                #outputs = model(inputs)\n",
        "                #_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                #train_total += labels.size(0)\n",
        "                #train_correct += (predicted.flatten().float() == labels.flatten()).sum().item()\n",
        "\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_gen():\n",
        "                labels = labels.view(-1, 1).float()\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                outputs = torch.sigmoid(outputs)\n",
        "                outputs = (outputs > 0.5).float()\n",
        "                test_correct += (outputs == labels).sum().item()\n",
        "                test_total += labels.size(0)\n",
        "\n",
        "                #outputs = model(inputs)\n",
        "                #_, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                #test_total += labels.size(0)\n",
        "                #train_correct += (predicted.flatten().float() == labels.flatten()).sum().item()\n",
        "\n",
        "        return (train_correct / train_total, test_correct / test_total)\n",
        "        \n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    criterion: Callable,\n",
        "    optimizier: torch.optim.Optimizer,\n",
        "    n_epochs: int,\n",
        "    train_gen: Callable,\n",
        "    valid_gen: Callable,\n",
        ") -> Tuple[List[float], List[float], nn.Module]:\n",
        "    \"\"\"Train a model using a batch gradient descent.\n",
        "    Args:\n",
        "        model: a class inheriting from nn.Module.\n",
        "        criterion: a loss criterion.\n",
        "        optimizer: an optimizer (e.g. Adam, RMSprop, ...).\n",
        "        n_epochs: the number of training epochs.\n",
        "        train_gen: a callable function returing a batch (data, labels).\n",
        "        valid_gen: a callable function returing a batch (data, labels).\n",
        "    \"\"\"\n",
        "    train_losses = np.zeros(n_epochs)\n",
        "    valid_losses = np.zeros(n_epochs)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        t0 = datetime.now()\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        model_saves = []\n",
        "\n",
        "        # Training loop.\n",
        "        for inputs, labels in train_gen():\n",
        "            # labels are of dimension (N,) we turn them into (N, 1).\n",
        "            labels = labels.view(-1, 1).float()\n",
        "            # Put them on the GPU.\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Reset the gradient.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizier.step()\n",
        "\n",
        "            train_loss.append(loss.item())  # .item() detach the value from GPU.\n",
        "            model_saves.append(copy.deepcopy(model))\n",
        "\n",
        "        train_losses[epoch] = np.mean(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = []\n",
        "        # Evaluation loop.\n",
        "        for inputs, labels in valid_gen():\n",
        "            labels = labels.view(-1, 1).float()\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            valid_loss.append(loss.item())\n",
        "\n",
        "        valid_losses[epoch] = np.mean(valid_loss)\n",
        "\n",
        "        (train_acc, test_acc) = accuracy(model, train_gen, valid_gen)\n",
        "\n",
        "        print(f\"Epoch: {epoch}, training loss: {train_losses[epoch]}, validation loss: {valid_losses[epoch]}, in {datetime.now() - t0}, train_acc: \" + str(train_acc)+\", val_acc: \" + str(train_acc))\n",
        "\n",
        "    val, idx = min((val, idx) for (idx, val) in enumerate(valid_losses))\n",
        "\n",
        "    return train_losses, valid_losses, model_saves[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm0H1cfBCaaA"
      },
      "source": [
        "We setup the model, criterion (a binary cross entropy), and the optimizer (Adam).\n",
        "\n",
        "Note that `BCEWithLogitsLoss` use a mathematical trick to incorporate the sigmoid function in its computation. This trick makes the learning process go slightly faster and is the reason why we didn't put a sigmoid in the forward function of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Mpe44Dd85Z"
      },
      "outputs": [],
      "source": [
        "model = RNN(len(vocabulary), 32, 64, 1, 1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset"
      ],
      "metadata": {
        "id": "feRRHWZAmMz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3vwDKlzCaaB"
      },
      "source": [
        "We get the 3 generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srYPmX_aeLwD"
      },
      "outputs": [],
      "source": [
        "train_gen = lambda: data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"])\n",
        "valid_gen = lambda: data_generator(encoded_dataset[\"validation\"][\"text\"], encoded_dataset[\"validation\"][\"label\"])\n",
        "test_gen = lambda: data_generator(encoded_dataset[\"test\"][\"text\"], encoded_dataset[\"test\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCPkSbC8CaaB"
      },
      "source": [
        "And train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB98HqD3fk1G"
      },
      "outputs": [],
      "source": [
        "train_losses, valid_losses, model = train(model, criterion, optimizer, 20, train_gen, valid_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gofYIVy8CaaC"
      },
      "source": [
        "We can look at the training and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VATw05GYfwNm"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label=\"Training loss\")\n",
        "plt.plot(valid_losses, label=\"Validation loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSFFyEtlCaaD"
      },
      "source": [
        "For the assignment, code the following.\n",
        "* **(2 points)** The model validation loss should go down and then up. It means the model starts overfitting after a certain number of iterations. Modify the `train` function so it returns the model found with the best validation loss.\n",
        "* **(2 points)** Add an accuracy function and report the accuracy of the training and test set.\n",
        "* **(3 points)** Create an LSTM class which uses an LSTM instead of an RNN. Compare its results with the RNN.\n",
        "* **(2 point)** Implement a function which takes any text and return the model's prediction.\n",
        "    * The function should have a string as input and return a class (0 or 1) and its probability (score out of a [sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html)).\n",
        "    * Don't forget to make the text go through the same pretreatment and encoding you used to train your model.\n",
        "* **(3 points)** Create a bidirectional LSTM (BiLSTM) class to classify your sentences. Report the accuracy on the training and test data.\n",
        "    * To combine the last output of both direction, you can concatenate, add, or max-pool them. Please document your choice.\n",
        "* **(1 point)** With your best classifier, look at two wrongly classified examples on the test set. Try explaining why the model was wrong.\n",
        "* **(Bonus)** Try finding better hyperparameters (dimensions, number of layers, ...). Document your experiments and results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"A simple LSTM module with word embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: vocabulary size.\n",
        "            embed_size: embedding dimensions.\n",
        "            hidden_size: hidden layer size.\n",
        "            n_layers: the number of layers.\n",
        "            n_outputs: the number of output classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_outputs = n_outputs\n",
        "\n",
        "\n",
        "        # The word embedding layer.\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        # The LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = self.embed_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.n_layers,\n",
        "            batch_first = True, # Changes the order of dimension to put the batches first.\n",
        "        )\n",
        "        # A fully connected layer to project the LSTM's output to only one output used for classification.\n",
        "        self.fc = nn.Linear(self.hidden_size, self.n_outputs)\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Function called when the model is called with data as input.\n",
        "        Args:\n",
        "            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n",
        "        Returns:\n",
        "            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        out = self.embed(X)\n",
        "        # out contains the output layer of all words in the sequence.\n",
        "        # First dim is batch, second the word in the sequence, third is the vector itself.\n",
        "        # The second output value is the last vector of all intermediate layer.\n",
        "        # Only use it if you want to access the intermediate layer values of a\n",
        "        # multilayer model.\n",
        "        out, _ = self.lstm(out, (h0,c0))\n",
        "        # Getting the last value only.\n",
        "        out = out[:, -1, :]\n",
        "    \n",
        "        # Linear projection.\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "4ZROc9P8b_5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM(len(vocabulary), 32, 64, 1, 1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Q11zSR4dciQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, valid_losses, model = train(model, criterion, optimizer, 10, train_gen, valid_gen)"
      ],
      "metadata": {
        "id": "tcB1fC0UcqQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5FZbgwK-yoX"
      },
      "outputs": [],
      "source": [
        "def model_generator(txt, model) -> int:\n",
        "  corrected = pretreatment(txt)\n",
        "  encoded = encode_text(corrected)\n",
        "  word_ind_tensor = torch.tensor(encoded, device=\"cpu\").view(1, -1).to(device)\n",
        "  out = model(word_ind_tensor)\n",
        "  out = torch.sigmoid(out)\n",
        "  out = (out > 0.5).float()\n",
        "  return out.item()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"I like this movie\"\n",
        "model_generator(test, model)"
      ],
      "metadata": {
        "id": "UlIKZFu2e3tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "\n",
        "   #def __init__(self, vocab_size: int, embed_size: int,   hidden_size: int,   n_layers: int, n_outputs: int)\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int,   batch_size : int , label_size: int, use_gpu = True):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.use_gpu = use_gpu\n",
        "        self.batch_size = batch_size\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True)\n",
        "        self.hidden2label = nn.Linear(hidden_dim*2, label_size)\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # first is the hidden h\n",
        "        # second is the cell c\n",
        "        if self.use_gpu:\n",
        "            return (Variable(torch.zeros(2, self.batch_size, self.hidden_dim).cuda()),\n",
        "                    Variable(torch.zeros(2, self.batch_size, self.hidden_dim).cuda()))\n",
        "        else:\n",
        "            return (Variable(torch.zeros(2, self.batch_size, self.hidden_dim)),\n",
        "                    Variable(torch.zeros(2, self.batch_size, self.hidden_dim)))\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        x = self.embeddings(sentence).view(len(sentence), self.batch_size, -1)\n",
        "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
        "        y = self.hidden2label(lstm_out[-1])\n",
        "        log_probs = F.log_softmax(y)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "Sc5Yj-J-unLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    \"\"\"A simple LSTM module with word embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: vocabulary size.\n",
        "            embed_size: embedding dimensions.\n",
        "            hidden_size: hidden layer size.\n",
        "            n_layers: the number of layers.\n",
        "            n_outputs: the number of output classes.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_outputs = n_outputs\n",
        "\n",
        "\n",
        "        # The word embedding layer.\n",
        "        self.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        # The LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size = self.embed_size,\n",
        "            hidden_size = self.hidden_size,\n",
        "            num_layers = self.n_layers,\n",
        "            batch_first = True, # Changes the order of dimension to put the batches first.\n",
        "            bidirectional=True\n",
        "        )\n",
        "        # A fully connected layer to project the LSTM's output to only one output used for classification.\n",
        "        self.fc = nn.Linear(self.hidden_size * 2, self.n_outputs)\n",
        "\n",
        "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Function called when the model is called with data as input.\n",
        "        Args:\n",
        "            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n",
        "        Returns:\n",
        "            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.n_layers * 2, X.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.n_layers * 2, X.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        out = self.embed(X)\n",
        "        # out contains the output layer of all words in the sequence.\n",
        "        # First dim is batch, second the word in the sequence, third is the vector itself.\n",
        "        # The second output value is the last vector of all intermediate layer.\n",
        "        # Only use it if you want to access the intermediate layer values of a\n",
        "        # multilayer model.\n",
        "\n",
        "        out, _ = self.lstm(out, (h0,c0))\n",
        "        # Getting the last value only.\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        # Linear projection.\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return torch.sum(out, 1).view(-1, 1)"
      ],
      "metadata": {
        "id": "GsYQfkg5_fdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BiLSTM(len(vocabulary), 32, 64, 1, 2).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "Sz4jwcKGvIIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, valid_losses, model = train(model, criterion, optimizer, 20, train_gen, valid_gen)"
      ],
      "metadata": {
        "id": "8hbrjVJnijhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On constate que BiLSTM a les meme resultats que LSTM mais qu'ils sont meilleurs que RNN."
      ],
      "metadata": {
        "id": "5BfMyRrI3sUD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "djwVgJ_634FH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}